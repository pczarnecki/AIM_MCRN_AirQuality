---
title: "R Notebook"
output: html_notebook
---

This file retrieves a time series of daily average temperature and relative humidity from NOAA stations closest to each specified city.


The first chunk gets the lattitude/longitude of all the cities
```{r}
library(RJSONIO)

#read in the list of cities
infection_data <- read_csv("italy2000.csv")
cities <- names(infection_data)
cities <- cities[2:length(cities)]

#prepare the output data table (columns with city name, latitude, and logitude)
output = infection_data[1:length(cities),2]
output$cities <- cities
output = output[1:length(cities),2] #now its just a list of cities


ncity <- length(cities)
counter <- 1
infection_data$lon[counter] <- 0
infection_data$lat[counter] <- 0
while (counter <= ncity){
  #Get lattitude and longitude from an api. This part is from stack exchange
  CityName <- gsub(' ','%20',output$cities[counter]) #remove space for URLs
  url <- paste(
    "http://nominatim.openstreetmap.org/search?city="
    , CityName
    , "&countrycodes=IT&limit=9&format=json"
    , sep="")
  x <- fromJSON(url)
  if(is.vector(x)){
    output$lon[counter] <- x[[1]]$lon
    output$lat[counter] <- x[[1]]$lat    
  }
  counter <- counter + 1
}
write.csv(output, "city_locations.csv")
```
At this stage you should check the latitude/longitudes actually match up with the correct cities.


The second chunk gets the code of the closest NOAA weather station to each city

```{r}
library(worldmet)
require(stringi)

city_data <- read.csv("city_locations.csv")
city_data <- city_data[1:nrow(city_data),2:ncol(city_data)]
city_data$NOAAcode[1] <- 0


#find and clean up NOAA codes
  meta <- getMetaLive()

  #Only consider italian stations
  id <- which(meta$CTRY %in% "IT")
  meta <- meta[id, ]

  #Only consider stations with 2020 data
  id <- which(format(meta$END, "%Y") %in% 2020)
  meta <- meta[id, ]
  
  #Get rid of stations that dont have temperature data (found empirically)
  id <- which(meta$code %in% "161320-99999")
  meta = meta[-id, ]
  id <- which(meta$code %in% "161210-99999")
  meta = meta[-id, ]
  id <- which(meta$code %in% "160680-99999")
  meta = meta[-id, ]
  id <- which(meta$code %in% "160230-99999")
  meta = meta[-id, ]
  id <- which(meta$code %in% "161010-99999")
  meta_final = meta[-id, ]

  
  n <- 1 #number of stations to be returned.
  r <- 6371 # radius of the Earth
  
for (i in 1:nrow(city_data)) {
  #lat and lon of each city
  lon <- city_data$lon[i]
  lat <- city_data$lat[i]


  meta_final$longR <- meta_final$LON * pi / 180
  meta_final$latR <- meta_final$LAT * pi / 180
  LON <- lon * pi / 180
  LAT <- lat * pi / 180
  #calculate the distance between each station and the   city
  meta_final$dist <- acos(sin(LAT) * sin(meta_final$latR) + cos(LAT) *cos(meta_final$latR) * cos(meta_final$longR - LON)) * r

  #sort and retrun top n nearest
  final_code <- head(openair:::sortDataFrame(meta_final, key = "dist"), n)
  city_data$NOAAcode[i] <- final_code$code
  
}

write.csv(city_data, "city_noaa_codes.csv")
```

The third chunk retrieves hourly temperature/humidity data from the NOAA database and finds the median for each day. Writes individual files for each city

```{r}
library(worldmet)
library(lubridate)
library(dplyr)
#Get our list of cities and NOAA codes
city_data <- read.csv("city_noaa_codes.csv")
city_data <- city_data[1:nrow(city_data),2:ncol(city_data)]


for (i in 1:nrow(city_data)) {
  #we only want to keep data for the date, temperature, and humidity
  keep <- c("date", "air_temp", "RH")
  data_noaa <- importNOAA(code = city_data$NOAAcode[i], year = 2020) 
  data_noaa = data_noaa[keep]
  data_noaa <- na.omit(data_noaa)
  #Group everything by day and find the medians
  data_noaa <- data_noaa %>% mutate(Day = day(date), Month = month(date))
  median_temp <- data_noaa %>% group_by(Day, Month) %>% summarize(med_temp = median(air_temp))
  median_hum <- data_noaa %>% group_by(Day, Month) %>% summarize(med_hum = median(RH))
  total_median <- merge(median_hum,median_temp,by=c("Day","Month"))
  
  #make a column with city names just for fun
  total_median$city <- city_data$cities[i]
  
  #format the date better
  total_median <- total_median %>% mutate(date = make_date(2020, total_median$Month, total_median$Day))
  keep <- c("date", "med_temp", "med_hum")
  total_median <- total_median[keep]
  total_median <- total_median[order(as.Date(total_median$date, format="%Y/%m/%d")),]
  
  #write to a folder called data (which must exist i think)
  write_csv(total_median, paste("data/", city_data$cities[i], ".csv", sep=""))
}
```



This last chunk combines the files that have the NOAA data with the files that have air quality/COVID/mobility data
```{r}
library(lubridate)
library(dplyr)
#read in our list of cities and their daily covid cases
city_names <- read.csv("city_noaa_codes.csv")
city_names <- city_names$cities
city_data <- read.csv("italy2000.csv")

for (i in 1:length(city_names)) {
  #read in the csv for our city
  city <- read_csv(paste("data/", city_names[i], ".csv", sep=""))
  
  #remove weather data before cases reported
  start_date = as.Date(make_date(2020, 2, 24), format = "%Y/%m/%d")
  end_date = as.Date(make_date(2020, 7, 23), format = "%Y/%m/%d")
  keep <- which(as.Date(city$date, format = "%Y/%m/%d") >= start_date)
  city <- city[keep,]
  keep <- which(as.Date(city$date, format = "%Y/%m/%d") <= end_date)
  city <- city[keep,]
  times <- seq.Date(start_date, end_date, "day")
  df <- data.frame(date=times)
  city_data$Date <- as.Date(city_data$Date, format = "%m/%d/%Y")
  city <- full_join(df, city)
  
  city$Daily <- city_data[[sub(" ", ".", city_names[i])]]
  write_csv(city, paste("data/", city_names[i], "_full.csv", sep=""))
}



```


